# Data Management Guidelines

## Overview

This document defines data organization patterns for the `agentic-ai-public` repository.

## Principles

1. **Agent Self-Containment**: Each agent should be independently distributable
2. **Shared Data Efficiency**: Common datasets should not be duplicated
3. **Git-Friendly**: Large data files must be excluded from version control
4. **Documentation First**: All datasets must have accompanying metadata

## Directory Structure

### Agent-Specific Data (Default)

Data used by **only one agent** lives within that agent:

```
tool_use/ml_agent/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ .gitignore              # Ignore *.csv, *.pkl, *.h5, etc.
â”‚   â”œâ”€â”€ README.md               # Data catalog for this agent
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ (gitignored data)
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ tcga_processed.csv  # gitignored
â”‚       â”œâ”€â”€ tcga_processed.md   # committed - describes the data
â”‚       â””â”€â”€ manifest.json       # committed - file checksums, sizes
```

### Shared Data (When Needed)

Data used by **multiple agents** lives in a shared location:

```
data/                           # Project root shared data
â”œâ”€â”€ .gitignore                  # Ignore all data files
â”œâ”€â”€ README.md                   # Central data catalog
â”œâ”€â”€ tcga/
â”‚   â”œâ”€â”€ tcga_processed.csv      # gitignored
â”‚   â”œâ”€â”€ tcga_processed.md       # committed
â”‚   â”œâ”€â”€ manifest.json           # committed
â”‚   â””â”€â”€ download.py             # Script to regenerate data
â””â”€â”€ financial/
    â”œâ”€â”€ stock_data.csv          # gitignored
    â”œâ”€â”€ stock_data.md           # committed
    â””â”€â”€ manifest.json           # committed
```

**Agents reference shared data via config:**

```python
# In agent config.py
SHARED_DATA_DIR = PROJECT_ROOT / "data"
TCGA_DATA = SHARED_DATA_DIR / "tcga" / "tcga_processed.csv"

# Fallback to local if shared doesn't exist
if not TCGA_DATA.exists():
    TCGA_DATA = ML_AGENT_DIR / "data" / "processed" / "tcga_processed.csv"
```

## File Patterns

### 1. Dataset Documentation (`.md`)

Every dataset CSV must have a companion `.md` file:

```markdown
# Dataset Name

## Overview
Brief description of the dataset.

## Statistics
- Samples: 500
- Features: 1000
- Classes: 5

## Schema
| Column | Type | Description |
|--------|------|-------------|
| GENE_00001 | float | Expression level |

## Provenance
- Source: TCGA via GDC
- Generated: 2024-11-06
- Script: `scripts/download_tcga_data.py`

## Usage
```python
import pandas as pd
df = pd.read_csv("tcga_processed.csv")
```

## Version History
- v1.0.0 (2024-11-06): Initial dataset
```

### 2. Manifest File (`manifest.json`)

Track file metadata without committing the files:

```json
{
  "version": "1.0.0",
  "generated": "2024-11-06T12:00:00Z",
  "files": {
    "tcga_processed.csv": {
      "size_bytes": 52428800,
      "rows": 500,
      "columns": 1001,
      "sha256": "abc123...",
      "description": "Processed TCGA gene expression data"
    },
    "selected_genes.txt": {
      "size_bytes": 8192,
      "lines": 1000,
      "sha256": "def456...",
      "description": "List of selected gene names"
    }
  },
  "regenerate_command": "python scripts/download_tcga_data.py"
}
```

### 3. Data Catalog (`README.md`)

Each data directory needs a catalog:

```markdown
# ML Agent Data Catalog

## Available Datasets

### TCGA Processed Expression Data
- **File**: `processed/tcga_processed.csv` (gitignored)
- **Documentation**: `processed/tcga_processed.md` (committed)
- **Size**: ~50 MB
- **Samples**: 500
- **Features**: 1000 genes
- **Classes**: 5 cancer types

**To regenerate:**
```bash
python scripts/download_tcga_data.py
```

### Feature Importance
- **File**: `models/feature_importance.csv`
- **Generated by**: `scripts/train_model.py`
- **Purpose**: Gene importance rankings
```

## `.gitignore` Rules

### Agent-Level `.gitignore`

```gitignore
# tool_use/ml_agent/data/.gitignore

# Ignore all data files
*.csv
*.tsv
*.parquet
*.h5
*.hdf5
*.pkl
*.pickle
*.npy
*.npz

# Ignore raw downloads
raw/**

# But keep documentation
!*.md
!manifest.json
!README.md
!.gitignore

# Keep small reference files if needed
!*_schema.csv
!*_sample.csv
```

### Root-Level `.gitignore`

```gitignore
# Add to root .gitignore

# Shared data directory (if created)
/data/**/*.csv
/data/**/*.pkl
/data/**/*.h5

# But keep documentation
!/data/**/*.md
!/data/**/manifest.json
!/data/**/README.md
!/data/**/.gitignore
```

## Best Practices

### 1. Documentation First

Before committing any data pipeline:
- [ ] Create dataset `.md` documentation
- [ ] Generate `manifest.json` with checksums
- [ ] Update data catalog `README.md`
- [ ] Add regeneration script or instructions

### 2. Naming Conventions

```
dataset_name.csv        # Data file (gitignored)
dataset_name.md         # Documentation (committed)
dataset_name_schema.csv # Schema reference (committed, small)
dataset_name_sample.csv # Sample rows (committed, <100 rows)
```

### 3. Size Guidelines

| Size | Action |
|------|--------|
| < 1 MB | Can commit if necessary |
| 1-10 MB | Gitignore + document |
| 10-100 MB | Gitignore + manifest + download script |
| > 100 MB | Gitignore + manifest + external storage link |

### 4. Regeneration Scripts

Every dataset should be reproducible:

```python
# scripts/download_tcga_data.py
def main():
    """
    Download and process TCGA data.
    
    Generates:
    - data/processed/tcga_processed.csv
    - data/processed/tcga_processed.md
    - data/processed/manifest.json
    """
    # ... download logic ...
    
    # Generate manifest
    generate_manifest(output_dir)
    
    # Generate documentation
    generate_documentation(output_dir, metadata)
```

## Migration Path

### Current State â†’ Hybrid Approach

1. **Keep current structure** for ml_agent (agent-specific data)
2. **Create shared `/data`** only when needed (e.g., multiple agents use TCGA)
3. **Add fallback logic** in configs to check both locations

### Example Migration

```python
# config.py
from pathlib import Path

PROJECT_ROOT = Path(__file__).parent.parent.parent
AGENT_DIR = Path(__file__).parent

# Try shared data first, fallback to local
def get_data_path(dataset_name: str) -> Path:
    """Get path to dataset, checking shared then local."""
    shared = PROJECT_ROOT / "data" / dataset_name
    local = AGENT_DIR / "data" / "processed" / dataset_name
    
    if shared.exists():
        return shared
    elif local.exists():
        return local
    else:
        raise FileNotFoundError(
            f"Dataset {dataset_name} not found in shared or local data dirs. "
            f"Run download script to generate."
        )

# Usage
TCGA_DATA = get_data_path("tcga_processed.csv")
```

## Tools

### Generate Manifest

```python
# scripts/generate_manifest.py
import json
import hashlib
from pathlib import Path
from datetime import datetime

def generate_manifest(data_dir: Path):
    """Generate manifest.json for data directory."""
    manifest = {
        "version": "1.0.0",
        "generated": datetime.utcnow().isoformat() + "Z",
        "files": {}
    }
    
    for file in data_dir.glob("*.csv"):
        manifest["files"][file.name] = {
            "size_bytes": file.stat().st_size,
            "sha256": compute_sha256(file),
            "description": f"Data file: {file.name}"
        }
    
    with open(data_dir / "manifest.json", "w") as f:
        json.dump(manifest, f, indent=2)

def compute_sha256(file_path: Path) -> str:
    """Compute SHA256 hash of file."""
    sha256 = hashlib.sha256()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            sha256.update(chunk)
    return sha256.hexdigest()
```

## Summary

**Default Pattern**: Agent-specific data (current ml_agent approach)

**When to Centralize**: Only when 2+ agents need the same dataset

**Always Commit**:
- âœ… `.md` documentation
- âœ… `manifest.json` metadata
- âœ… `README.md` catalogs
- âœ… Download/regeneration scripts

**Never Commit**:
- âŒ Large CSV/binary data files
- âŒ Model checkpoints (unless tiny)
- âŒ Raw downloaded data

This keeps the repo **git-friendly** while maintaining **agent portability**! ğŸ¯
